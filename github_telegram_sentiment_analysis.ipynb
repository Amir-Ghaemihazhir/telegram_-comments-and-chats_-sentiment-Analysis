{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvC_AKoTUSx-"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "from hazm import Normalizer, word_tokenize, stopwords_list\n",
        "from collections import defaultdict\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# لیست حروف اضافه فارسی و کلمات اضافی\n",
        "stop_words = set(stopwords_list())\n",
        "custom_stop_words = set(['سلام', 'ببخشید', 'لطفا', 'گروه'])  # کلمات مورد نظر خود را اینجا اضافه کنید\n",
        "\n",
        "# ترکیب دو لیست\n",
        "all_stop_words = stop_words.union(custom_stop_words)\n",
        "\n",
        "\n",
        "########################\n",
        "def read_json_file(file_path):\n",
        "  \"\"\"Reads a JSON file and returns its content as a Python dictionary.\"\"\"\n",
        "  with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "  return data\n",
        "\n",
        "# Replace 'folder/AKHARINKHABAR.json' with the actual path to your JSON file\n",
        "file_path = 'your file path.json'\n",
        "data = read_json_file(file_path)\n",
        "###################################################\n",
        "# ابزارهای hazm را راه‌اندازی کنید\n",
        "normalizer = Normalizer()\n",
        "\n",
        "# احراز هویت Hugging Face\n",
        "token = \"your Hugging Face token-go to your hugging face account and https://huggingface.co/settings/tokens/new?\"\n",
        "\n",
        "# مدل تحلیل احساسات را بارگیری کنید\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-fa-base-uncased-sentiment-digikala\", token=token)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"HooshvareLab/bert-fa-base-uncased-sentiment-digikala\", token=token)\n",
        "\n",
        "# شمارش پیام‌های هر کاربر\n",
        "user_message_counts = defaultdict(int)\n",
        "user_messages = defaultdict(list)\n",
        "\n",
        "# توابع تحلیل احساسات\n",
        "def get_sentiment(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "    outputs = model(**inputs)\n",
        "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "    return torch.argmax(probs, dim=-1).item(), probs.max().item()\n",
        "\n",
        "sentiment_labels = [\"منفی\", \"خنثی\", \"مثبت\"]\n",
        "\n",
        "# تحلیل احساسات پیام‌ها و جمع‌بندی احساسات هر کاربر\n",
        "user_sentiments = defaultdict(list)\n",
        "\n",
        "for message in data['messages']:\n",
        "    if 'from' in message and 'text' in message:\n",
        "        user_id = message['from']\n",
        "        if isinstance(message['text'], list):\n",
        "            text = ' '.join([str(item) for item in message['text'] if isinstance(item, str)])\n",
        "        else:\n",
        "            text = str(message['text'])\n",
        "\n",
        "        normalized_text = normalizer.normalize(text)\n",
        "        user_message_counts[user_id] += 1\n",
        "        user_messages[user_id].append(normalized_text)\n",
        "\n",
        "        # تحلیل احساسات پیام\n",
        "        sentiment, confidence = get_sentiment(normalized_text)\n",
        "        user_sentiments[user_id].append((sentiment_labels[sentiment], confidence))\n",
        "\n",
        "# فیلتر کردن کاربران با بیش از 50 پیام\n",
        "active_users = {user_id: texts for user_id, texts in user_messages.items() if user_message_counts[user_id] > 1}\n",
        "\n",
        "# جمع‌بندی احساسات برای کاربران فعال\n",
        "user_summary = {}\n",
        "\n",
        "for user_id, sentiments in user_sentiments.items():\n",
        "    if user_id in active_users:\n",
        "        sentiment_counts = defaultdict(int)\n",
        "        for sentiment, _ in sentiments:\n",
        "            sentiment_counts[sentiment] += 1\n",
        "\n",
        "        total_messages = sum(sentiment_counts.values())\n",
        "        user_summary[user_id] = {sentiment: count / total_messages for sentiment, count in sentiment_counts.items()}\n",
        "\n",
        "# رسم نمودار ورد کلود برای هر کاربر\n",
        "for user_id, messages in active_users.items():\n",
        "    # ترکیب پیام‌های کاربر به یک متن طولانی\n",
        "    combined_text = ' '.join(messages)\n",
        "\n",
        "    # حذف کلمات توقف\n",
        "    words = word_tokenize(combined_text)\n",
        "    filtered_words = [word for word in words if word not in all_stop_words]\n",
        "    filtered_text = ' '.join(filtered_words)\n",
        "\n",
        "    # ایجاد ورد کلود\n",
        "    wordcloud = WordCloud(font_path='/your farsi font path.ttf', width=800, height=400, background_color='white').generate(filtered_text)\n",
        "\n",
        "    # نمایش ورد کلود\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(f'Word Cloud for User {user_id}')\n",
        "    plt.show()\n",
        "\n",
        "    # نمایش جمع‌بندی احساسات\n",
        "    print(f'Sentiment Summary for User {user_id}:')\n",
        "    for sentiment, ratio in user_summary[user_id].items():\n",
        "        print(f'  {sentiment}: {ratio:.2%}')\n"
      ]
    }
  ]
}